#!/bin/bash

#WORKINGDIR="/tmp/${0##*/}-$$"
WORKINGDIR=".${0##*/}-$$"

if (( ${#@} != 1 )) || [[ $1 = '--help' ]]; then
	printf '%s\n' \
		"Renders PDF files of books on Scribd." \
		"Usage: ${0##*/} scribd_book_url"
	exit 1
elif ! command -v wget > /dev/null; then
	echo "This script requires wget."
	exit 1
elif ! command -v convert > /dev/null; then
	echo "This script requires ImageMagick."
	exit 1
elif ! command -v pdftk > /dev/null; then
	echo "This script requires PDFTK."
	exit 1
fi

printf 'Getting HTML content... '
if ! html="$(wget -qO - "$1")"; then
	printf '%s\n' "error." "Could not download \"$1\"."
	exit 1
fi
echo "done."

filename="${html#*<title>}"
filename="${filename%%</title>*}"
filename="${filename//\//-}"

echo "Destination PDF: \"$filename.pdf\"."

printf 'Searching for assetPrefix... '
regexPrefix="docManager.assetPrefix = \""
if ! [[ $html =~ $regexPrefix ]]; then
	printf '%s\n' "error." "\"$1\" is not a valid Scribd book URL."
	exit 1
fi
prefix="${html#*$regexPrefix}"
prefix="${prefix%%\"*}"
echo "\"$prefix\"."

printf 'Finding pages to download... '
regexJpg="http://html.scribd.com/$prefix/images/[0-9]+-[0-9a-z]+.jpg"
regexJsonp="http://html[1-4].scribdassets.com/$prefix/pages/[0-9]+-[0-9a-z]+.jsonp"
unset -v pages
while read line; do
	if [[ $line =~ $regexJsonp ]]; then
		page="${line##*/pages/}"
		page="${page%%.jsonp*}"
		pages+=("$page")
		continue
	fi

	if [[ $line =~ $regexJpg ]]; then
		page="${line##*/images/}"
		page="${page%%.jpg*}"
		pages+=("$page")
	fi
done <<< "$html"
totalPages="${#pages[@]}"
echo "$totalPages pages."

cleanup() {  
	printf 'Cleaning up... '
	rm -r "$WORKINGDIR"
	echo "done."
}

trap 'echo; clean-up; exit 1' SIGINT SIGTERM

echo "Using working directory \"$WORKINGDIR\"."
mkdir "$WORKINGDIR"

padding="$(($totalPages-1))"
padding="${#totalPages}"
for pageIndex in ${!pages[@]}; do
	pageNumber="$((pageIndex + 1))"
	printf -v file "%.${padding}d" "$((pageIndex + 1))"

	printf 'Rendering PDF data for page %s (of %s)... ' "$pageNumber" "$totalPages"
	retries=0
	until (( retries == 10 )); do
		wget -qO "$WORKINGDIR/$file.jpg" "http://html.scribd.com/$prefix/images/${pages[$pageIndex]}.jpg" && break
		sleep 2
	done

	if (( retries == 10 )); then
		printf '%s\n' "error." "Error while downloading \"http://html.scribd.com/$prefix/images/${pages[$pageIndex]}.jpg\"."
		cleanup
		exit 1
	fi

	if ! convert "$WORKINGDIR/$file.jpg" "$WORKINGDIR/$file.pdf"; then
		printf '%s\n' "error." "Could not convert \"$WORKINGDIR/$file.jpg\" to PDF."
		cleanup
		exit 1
	fi

	rm "$WORKINGDIR/$file.jpg"
	echo "$(($(stat --printf '%s' "$WORKINGDIR/$file.pdf") / 1024)) KB."
done

printf 'Combining all PDF files to one file... '
pdftk "$WORKINGDIR/"*".pdf" cat output "$WORKINGDIR/$filename.pdf"
mv "$WORKINGDIR/$filename.pdf" "$filename.pdf"
echo "$(($(stat --printf '%s' "$filename.pdf") / 1024)) KB."

cleanup

echo "PDF file saved as \"$filename.pdf\"."
